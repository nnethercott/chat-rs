syntax="proto3";
package inferenceservice;

import "google/protobuf/empty.proto";
import "google/protobuf/wrappers.proto";

// api we'd like from an llm service
service Inferencer{
  rpc ListModels(google.protobuf.Empty) returns (stream google.protobuf.StringValue);
  // rpc runBatchedInference(stream InferenceRequest) returns (stream InferenceResponse);
  rpc AddModels(stream google.protobuf.StringValue) returns (google.protobuf.UInt64Value);
  rpc Generate(InferenceRequest) returns (InferenceResponse);
  rpc GenerateStreaming(google.protobuf.StringValue) returns (stream google.protobuf.StringValue);
}

message SamplingOpts{
  uint32 max_new_tokens = 1;
  repeated string eos_tokens = 2;
  optional uint32 top_k = 3;
  optional double top_p = 4;
  optional double temperature = 5;
  optional float repeat_penalty = 6;
}

enum Role{
  USER = 0;
  AGENT = 1;
  SYSTEM = 2;
}

message Turn{
  Role role = 1; 
  oneof data{
    string text = 2;
    bytes image_bytes = 3;
  }
}

message InferenceRequest{
  string model_id = 1;
  string prompt = 2;
  repeated Turn messages = 3;
  SamplingOpts opts = 4;
}

message Usage{
    int32 completion_tokens = 1;
    int32 prompt_tokens = 2;
  }

message InferenceResponse{
  int32 timestamp = 1;
  Usage usage = 2;
  string response = 3;
  repeated double logprobs = 4;
}
